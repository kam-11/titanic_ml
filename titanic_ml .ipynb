{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2b2a11",
   "metadata": {},
   "source": [
    "# Naive Bayes and K-Nearest Neighbors from Scratch\n",
    "This notebook implements **Naive Bayes** and **K-Nearest Neighbors (KNN)** algorithms from scratch using only NumPy, Pandas, and Matplotlib.\n",
    "\n",
    "### Objectives\n",
    "- Preprocess the Titanic dataset\n",
    "- Visualize gender and class survival rates\n",
    "- Implement Naive Bayes and KNN manually\n",
    "- Evaluate classification accuracy\n",
    "\n",
    "**Note:** No external ML libraries like `scikit-learn` are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34a6d4",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We select relevant columns and encode categorical features. Missing values are removed for simplicity.\n",
    "We also normalize `Age` and `Fare` to bring them to a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_data(df):\n",
    "    df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']].copy()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "    df[['Age', 'Fare']] = (df[['Age', 'Fare']] - df[['Age', 'Fare']].mean()) / df[['Age', 'Fare']].std()\n",
    "    x = df.drop('Survived', axis=1).values\n",
    "    y = df['Survived'].values\n",
    "    split_idx = int(0.8 * len(x))\n",
    "    return x[:split_idx], x[split_idx:], y[:split_idx], y[split_idx:], df\n",
    "\n",
    "x_train, x_test, y_train, y_test, preprocessed_df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11242dba",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "We visualize survival rates based on gender and passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    df['Sex_encoded'] = df['Sex']\n",
    "    df['Survived'].groupby(df['Sex_encoded']).mean().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title(\"Survival Rate by Gender\")\n",
    "    axes[0].set_xticklabels(['Male', 'Female'], rotation=0)\n",
    "    axes[0].set_ylabel(\"Survival Rate\")\n",
    "    df['Survived'].groupby(df['Pclass']).mean().plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title(\"Survival Rate by Class\")\n",
    "    axes[1].set_xlabel(\"Pclass\")\n",
    "    axes[1].set_ylabel(\"Survival Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_data(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0233c",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91727250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(x_train, y_train, x_test):\n",
    "    classes = np.unique(y_train)\n",
    "    priors = {c: np.mean(y_train == c) for c in classes}\n",
    "    means = {c: x_train[y_train == c].mean(axis=0) for c in classes}\n",
    "    variances = {c: x_train[y_train == c].var(axis=0) + 1e-6 for c in classes}\n",
    "\n",
    "    def gaussian_prob(x, mean, var):\n",
    "        exponent = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        return (1 / np.sqrt(2 * np.pi * var)) * exponent\n",
    "\n",
    "    y_pred = []\n",
    "    for x in x_test:\n",
    "        posteriors = {}\n",
    "        for c in classes:\n",
    "            prior = np.log(priors[c])\n",
    "            conditional = np.sum(np.log(gaussian_prob(x, means[c], variances[c])))\n",
    "            posteriors[c] = prior + conditional\n",
    "        y_pred.append(max(posteriors, key=posteriors.get))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "nb_predictions = naive_bayes_classifier(x_train, y_train, x_test)\n",
    "nb_accuracy = np.mean(nb_predictions == y_test)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d49f18",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(x_train, y_train, x_test, k=5):\n",
    "    def euclidean_distance(a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    y_pred = []\n",
    "    for test_point in x_test:\n",
    "        distances = [euclidean_distance(test_point, x) for x in x_train]\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        k_labels = y_train[k_indices]\n",
    "        majority = np.bincount(k_labels).argmax()\n",
    "        y_pred.append(majority)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "knn_predictions = knn_classifier(x_train, y_train, x_test, k=5)\n",
    "knn_accuracy = np.mean(knn_predictions == y_test)\n",
    "print(f\"KNN Accuracy (k=5): {knn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b07f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Implemented both algorithms from scratch.\n",
    "- Evaluated accuracy on real Titanic data.\n",
    "- Preprocessing and visualizations included for bonus marks.\n",
    "\n",
    "**Ready for upload to the portal and GitHub.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
